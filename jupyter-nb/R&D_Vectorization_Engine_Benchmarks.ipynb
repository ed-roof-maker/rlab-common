{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb41323",
   "metadata": {},
   "source": [
    "# Vectorization Design POC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e94e62",
   "metadata": {},
   "source": [
    "An R&D exploration by Edmond den Dekker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9b871",
   "metadata": {},
   "source": [
    "## Pandas Rolling Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c828e",
   "metadata": {},
   "source": [
    "The benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0c928f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import rlab_common_numpy_pandas as rlnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a23f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest as ut\n",
    "data = [2, 2, 3, 4, 5, 5, 5, 5, 6, 7, 8, 5, 4, 3, 4, 5, 3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4118d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, 7.0, 9.0, 12.0, 14.0, 15.0, 15.0, 16.0, 18.0, 21.0, 20.0, 17.0, 12.0, 11.0, 12.0, 12.0, 10.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "rolling_version = df.rolling(3).sum()\n",
    "print(list(rolling_version[0].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdfdd2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 µs ± 13.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit df.rolling(3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879596d",
   "metadata": {},
   "source": [
    "## Vectorized Rolling Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6ce5d",
   "metadata": {},
   "source": [
    "### Pros\n",
    "* The bigger the data, the faster the speed gains\n",
    "* Always faster than pandas - between 6 to 80 times faster\n",
    "\n",
    "### Cons\n",
    "* Complex code and way of thinking\n",
    "* Less runtime parameterizations for functions, eg. rolling windows are hard coded\n",
    "* Attempts to use exec() for parameterization will make it slower by a factor of 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d716b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class VectorizeEngine():\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\" If no specific library is set, then one will be selected during runtime\n",
    "            in the below order of precedence. It is best to pre-import a gpu library\n",
    "            and feed it in via this instance super constructor. This will be faster\n",
    "            if you have many vectorization layers\n",
    "\n",
    "            INPUT: **kwargs - set specific gpu / cpu library\n",
    "                            - use_tensorflow_lib=<tf module import object>\n",
    "                            - use_pytorch_lib=<pytorch module import object>\n",
    "                            - use_numpy_lib=<numpy module import object>\n",
    "        \"\"\"\n",
    "        self.__shift_direction = None\n",
    "        self.__TF = None\n",
    "        self.__PT = None\n",
    "        self.__NP = None\n",
    "        self.__DROPNA = False\n",
    "        if 'use_tensorflow_lib' in kwargs.keys():\n",
    "            self.__TF = kwargs['use_tensorflow_lib']\n",
    "        elif 'use_pytorch_lib' in kwargs.keys():\n",
    "            self.__PT = kwargs['use_pytorch_lib']\n",
    "        elif 'use_numpy_lib' in kwargs.keys():\n",
    "            self.__NP = kwargs['use_numpy_lib']\n",
    "        if 'dropna' in kwargs.keys():\n",
    "            self.__DROPNA = kwargs['dropna']\n",
    "\n",
    "    def _shiftr(self, data):\n",
    "        x = data.copy()\n",
    "        x.insert(0, None)\n",
    "        x.pop()\n",
    "        return x\n",
    "\n",
    "    def _shiftl(self, data):\n",
    "        x = data.copy()\n",
    "        x.append(None)\n",
    "        x.pop(0)\n",
    "        return x\n",
    "\n",
    "    def _shift(self, data, x):\n",
    "        if x > 0:\n",
    "            d = 1\n",
    "            e = x\n",
    "        else:\n",
    "            d = -1\n",
    "            e = abs(x)\n",
    "        if type(data) == tuple:\n",
    "            data = list(data)\n",
    "        new = data.copy()\n",
    "        for i in range(0, e, d):\n",
    "            if d == 1:\n",
    "                new = self._shiftr(new)\n",
    "            else:\n",
    "                new = self._shiftl(new)\n",
    "        return new\n",
    "\n",
    "    def _vshift(self, data, start, amount):\n",
    "        \"\"\" Abstracted method to vertically shift the given array N times.\n",
    "            If N is positive it shifts to the right, else to the left.\n",
    "\n",
    "            Use this method to take an array input and create a rolling value matrix for vectorization.\n",
    "            \n",
    "            INPUT: data - a single array\n",
    "                   start - usually 0\n",
    "                   amount - +/- integer - shift left or right by this much\n",
    "            OUTPUT: an array of arrays suitable for rolling vectorizations.\n",
    "        \"\"\"\n",
    "        return tuple([self._shift(data, x) for x in range(start, amount)])\n",
    "\n",
    "    def _dropna(self, data):\n",
    "        \"\"\" Abstracted method to exclude Nones from an array.\n",
    "        \"\"\"\n",
    "        return [x for x in data if x]\n",
    "\n",
    "    def _cropna(self, data):\n",
    "        \"\"\" Determines if the array was shifted left or right. Then we make sure the first array has\n",
    "            no Nones by dropping it. We then cut all the other arrays by the same amount so that it\n",
    "            is all an evenly cropped square.\n",
    "            \n",
    "            We crop the array matrix at either the left or right side.\n",
    "            \n",
    "            INPUT: array of arrays with values\n",
    "            OUTPUT: the same as the input but with cropped and squared values based on whether it was\n",
    "                    shifted right or left.\n",
    "        \"\"\"\n",
    "        data_size = data.__len__()\n",
    "        is_shifted_left = 0\n",
    "        is_shifted_right = 0\n",
    "        for datum in data:\n",
    "            first = datum[0]\n",
    "            last = datum[-1]\n",
    "            if not first:\n",
    "                is_shifted_right += 1\n",
    "            elif not last:\n",
    "                is_shifted_left += 1\n",
    "        first_array = data[0]\n",
    "        first_offset_size = 0\n",
    "        for i in first_array:\n",
    "            if not i:\n",
    "                first_offset_size += 1\n",
    "        cropped = []\n",
    "        if is_shifted_right > is_shifted_left:\n",
    "            self.__shift_direction = 1\n",
    "            for datum in data:\n",
    "                new = datum[first_offset_size:].copy()\n",
    "                for a in range(0, data_size - 1): new.pop(0)\n",
    "                cropped.append(new)\n",
    "        elif is_shifted_left > is_shifted_right:\n",
    "            self.__shift_direction = -1\n",
    "            for datum in data:\n",
    "                new = datum[first_offset_size:].copy()\n",
    "                for a in range(0, data_size - 1): new.pop()\n",
    "                cropped.append(new)\n",
    "        elif is_shifted_left == 0 and is_shifted_right == 0:\n",
    "            return tuple(data)\n",
    "        else:\n",
    "            raise ValueError('Bad data structure')\n",
    "        return tuple(cropped)\n",
    "\n",
    "    def __square(self, square_size, data):\n",
    "        \"\"\" Helper method for squaring a matrix.\n",
    "        \"\"\"\n",
    "        new_data = []\n",
    "        for series in data:\n",
    "            series_size = series.__len__()\n",
    "            fill_size = square_size - series_size\n",
    "            if fill_size == 0:\n",
    "                new_data.append(list(series))\n",
    "                continue\n",
    "            new_series = [None for x in range(0, fill_size)]\n",
    "            new_series.extend(list(series))\n",
    "            new_data.append(new_series)\n",
    "        return new_data\n",
    "    \n",
    "    def _square_right(self, data):\n",
    "        \"\"\" Pads an array of series so that it is shifted to the right \n",
    "            and padded with Nones to the left. The result should be a\n",
    "            squared matrix of values.\n",
    "            \n",
    "            This first array value should have no Nones at the left.\n",
    "\n",
    "            INPUT:  data     - an array of series data\n",
    "            OUTPUT: new_data - a squared matrix\n",
    "\n",
    "        \"\"\"\n",
    "        square_size = 0\n",
    "        for series in data:\n",
    "            size = series.__len__()\n",
    "            if size > square_size:\n",
    "                square_size = size\n",
    "        # square size is the largest series size\n",
    "        if square_size == 0:\n",
    "            raise ValueError('Data needs to be an array of arrays greater than 2.')\n",
    "        return self.__square(square_size, data)\n",
    "\n",
    "    def _square_right_by_ref(self, reference, data):\n",
    "        \"\"\" Instead of using the largest series as the reference, we use the specified\n",
    "            reference for squaring to the right.\n",
    "        \"\"\"\n",
    "        square_size = reference.__len__()\n",
    "        if self._is_matrices(data):\n",
    "            return self.__square(square_size, data)\n",
    "        elif self._is_series(data):\n",
    "            return self.__square(square_size, [data])[0]\n",
    "        else:\n",
    "            raise ValueError('Bad data type')\n",
    "    \n",
    "    def _has_accel_lib(self, lib='at-least-one'):\n",
    "        \"\"\" INPUT: lib - str - default - all\n",
    "                    - specific lib - tensorflow, pytorch, numpy\n",
    "            OUTPUT: boolean - True or False\n",
    "        \"\"\"\n",
    "        if lib == 'at-least-one':\n",
    "            if not self.__TF and not self.__PT and not self.__NP:\n",
    "                has_tf = self._has_accel_lib(lib='tensorflow')\n",
    "                has_pt = self._has_accel_lib(lib='pytorch')\n",
    "                has_np = self._has_accel_lib(lib='numpy')\n",
    "                if has_tf or has_pt or has_np:\n",
    "                    return True\n",
    "                return False\n",
    "            elif self.__TF:\n",
    "                return True\n",
    "            elif self.__PT:\n",
    "                return True\n",
    "            elif self.__NP:\n",
    "                return True\n",
    "        elif lib == 'tensorflow' and self.__TF:\n",
    "            return True\n",
    "        elif lib == 'pytorch' and self.__PT:\n",
    "            return True\n",
    "        elif lib == 'numpy' and self.__NP:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _is_series(self, data):\n",
    "        if type(data) != list or data.__len__() == 0:\n",
    "            return True\n",
    "        number_types = [int, float]\n",
    "        array_types = [list, tuple, np.array]\n",
    "        if type(data) in array_types and type(data[0]) not in array_types:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _is_matrices(self, data):\n",
    "        if type(data) != list or data.__len__() == 0:\n",
    "            return False\n",
    "        number_types = [int, float]\n",
    "        array_types = [list, tuple, np.array]\n",
    "        if type(data) in array_types and type(data[0]) in array_types:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _is_a_list_of_series(self, data):\n",
    "        \"\"\" Check if data is a list of series, or a list of matrices.\n",
    "        \"\"\"\n",
    "        number_types = [int, float]\n",
    "        array_types = [list, tuple, np.array]\n",
    "        if type(data[0][0]) in number_types or type(data[0][0]) not in array_types:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _is_a_list_of_matrices(self, data):\n",
    "        \"\"\" Check if data is a list of series, or a list of matrices.\n",
    "        \"\"\"\n",
    "        array_types = [list, tuple]\n",
    "        if type(data[0][0]) in array_types:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _multiply(self, data):\n",
    "        \"\"\" Multi library mutliply.\n",
    "        \"\"\"\n",
    "        if self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_matrices(data):\n",
    "            tf = self.__TF\n",
    "            new_data = []\n",
    "            for matrix in data:\n",
    "                new_matrix = []\n",
    "                for mrow in matrix:\n",
    "                    new_matrix.append(tuple(mrow))\n",
    "                new_data.append(np.array(new_matrix))\n",
    "            reduction = tf.constant(new_data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for matrix in new_data[1:]:\n",
    "                    reduction = tf.matmul(reduction, tf.constant(matrix))\n",
    "            new_reduction = []\n",
    "            for matrix in new_reduction:\n",
    "                new_reduction.append(list(matrix))\n",
    "            return new_reduction\n",
    "\n",
    "        elif self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_series(data):\n",
    "            tf = self.__TF\n",
    "            reduction = tf.constant(data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for layer in data[1:]:\n",
    "                    reduction = sess.run(tf.multiply(reduction, tf.constant(layer)))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='pytorch') and self._is_a_list_of_series(data):\n",
    "            pt = self.__PT\n",
    "            reduction = pt.tensor(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = pt.multiply(reduction, pt.tensor(layer))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='numpy') and self._is_a_list_of_series(data):\n",
    "            np = self.__NP\n",
    "            reduction = np.array(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = np.multiply(reduction, np.array(layer))\n",
    "            return list(reduction)\n",
    "        raise RuntimeError('Accellerator library requried.')\n",
    "\n",
    "    def _add(self, data):\n",
    "        \"\"\" Multi library add\n",
    "        \"\"\"\n",
    "        if self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_matrices(data):\n",
    "            tf = self.__TF\n",
    "            new_data = []\n",
    "            for matrix in data:\n",
    "                new_matrix = []\n",
    "                for mrow in matrix:\n",
    "                    new_matrix.append(tuple(mrow))\n",
    "                new_data.append(np.array(new_matrix))\n",
    "            reduction = tf.constant(new_data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for matrix in new_data[1:]:\n",
    "                    reduction = sess.run(tf.add(reduction, tf.constant(matrix)))\n",
    "            new_reduction = []\n",
    "            for matrix in new_reduction:\n",
    "                new_reduction.append(list(matrix))\n",
    "            return new_reduction\n",
    "        \n",
    "        elif self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_series(data):\n",
    "            tf = self.__TF\n",
    "            reduction = tf.constant(data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for layer in data[1:]:\n",
    "                    reduction = sess.run(tf.add(reduction, tf.constant(layer)))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='pytorch') and self._is_a_list_of_series(data):\n",
    "            pt = self.__PT\n",
    "            reduction = pt.tensor(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = pt.add(reduction, pt.tensor(layer))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='numpy') and self._is_a_list_of_series(data):\n",
    "            np = self.__NP\n",
    "            reduction = np.array(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = np.add(reduction, np.array(layer))\n",
    "            return list(reduction)\n",
    "        raise RuntimeError('Accellerator library requried.')\n",
    "\n",
    "    def _subtract(self, data):\n",
    "        \"\"\" Multi library subtract\n",
    "        \"\"\"\n",
    "        if self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_matrices(data):\n",
    "            tf = self.__TF\n",
    "            new_data = []\n",
    "            for matrix in data:\n",
    "                new_matrix = []\n",
    "                for mrow in matrix:\n",
    "                    new_matrix.append(tuple(mrow))\n",
    "                new_data.append(np.array(new_matrix))\n",
    "            reduction = tf.constant(new_data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for matrix in new_data[1:]:\n",
    "                    reduction = sess.run(tf.subtract(reduction, tf.constant(matrix)))\n",
    "            new_reduction = []\n",
    "            for matrix in new_reduction:\n",
    "                new_reduction.append(list(matrix))\n",
    "            return new_reduction\n",
    "\n",
    "        elif self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_series(data):\n",
    "            tf = self.__TF\n",
    "            reduction = tf.constant(data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for layer in data[1:]:\n",
    "                    reduction = sess.run(tf.subtract(reduction, tf.constant(layer)))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='pytorch') and self._is_a_list_of_series(data):\n",
    "            pt = self.__PT\n",
    "            reduction = pt.tensor(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = pt.subtract(reduction, pt.tensor(layer))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='numpy') and self._is_a_list_of_series(data):\n",
    "            np = self.__NP\n",
    "            reduction = np.array(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = reduction - np.array(layer)\n",
    "            return list(reduction)\n",
    "        raise RuntimeError('Accellerator library requried.')\n",
    "\n",
    "    def _divide(self, data):\n",
    "        \"\"\" Multi library divide\n",
    "        \"\"\"\n",
    "        if self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_matrices(data):\n",
    "            tf = self.__TF\n",
    "            new_data = []\n",
    "            for matrix in data:\n",
    "                new_matrix = []\n",
    "                for mrow in matrix:\n",
    "                    new_matrix.append(tuple(mrow))\n",
    "                new_data.append(np.array(new_matrix))\n",
    "            reduction = tf.constant(new_data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for matrix in new_data[1:]:\n",
    "                    reduction = sess.run(tf.divide(reduction, tf.constant(matrix)))\n",
    "            new_reduction = []\n",
    "            for matrix in new_reduction:\n",
    "                new_reduction.append(list(matrix))\n",
    "            return new_reduction\n",
    "\n",
    "        elif self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_series(data):\n",
    "            tf = self.__TF\n",
    "            reduction = tf.constant(data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for layer in data[1:]:\n",
    "                    reduction = sess.run(tf.divide(reduction, tf.constant(layer)))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='pytorch') and self._is_a_list_of_series(data):\n",
    "            pt = self.__PT\n",
    "            reduction = pt.tensor(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = pt.divide(reduction, pt.tensor(layer))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='numpy') and self._is_a_list_of_series(data):\n",
    "            np = self.__NP\n",
    "            reduction = np.array(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = reduction / np.array(layer)\n",
    "            return list(reduction)\n",
    "        raise RuntimeError('Accellerator library requried.')\n",
    "    \n",
    "    def _power(self, data):\n",
    "        \"\"\" Multi library power\n",
    "        \"\"\"\n",
    "        if self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_matrices(data):\n",
    "            raise RuntimeError('Power does not currently work with tensorflow')\n",
    "            tf = self.__TF\n",
    "            new_data = []\n",
    "            for matrix in data:\n",
    "                new_matrix = []\n",
    "                for mrow in matrix:\n",
    "                    new_matrix.append(tuple(mrow))\n",
    "                new_data.append(np.array(new_matrix))\n",
    "            reduction = tf.constant(new_data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for matrix in new_data[1:]:\n",
    "                    reduction = sess.run(tf.pow(reduction, tf.constant(matrix)))\n",
    "            new_reduction = []\n",
    "            for matrix in new_reduction:\n",
    "                new_reduction.append(list(matrix))\n",
    "            return new_reduction\n",
    "\n",
    "        elif self._has_accel_lib(lib='tensorflow') and self._is_a_list_of_series(data):\n",
    "            raise RuntimeError('Power does not currently work with tensorflow')\n",
    "            tf = self.__TF\n",
    "            reduction = tf.constant(data[0])\n",
    "            with tf.Session() as sess:\n",
    "                for layer in data[1:]:\n",
    "                    reduction = sess.run(tf.pow(reduction, tf.constant(layer)))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='pytorch') and self._is_a_list_of_series(data):\n",
    "            pt = self.__PT\n",
    "            reduction = pt.tensor(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = pt.pow(reduction, pt.tensor(layer))\n",
    "            return list(reduction)\n",
    "\n",
    "        elif self._has_accel_lib(lib='numpy') and self._is_a_list_of_series(data):\n",
    "            np = self.__NP\n",
    "            reduction = np.array(data[0])\n",
    "            for layer in data[1:]:\n",
    "                reduction = np.power(reduction, np.array(layer))\n",
    "            return list(reduction)\n",
    "        raise RuntimeError('Accellerator library requried.')\n",
    "\n",
    "    def _mode_np_helper(self, x, y, m):\n",
    "        if x == m:\n",
    "            return y\n",
    "\n",
    "    def _mode_np(self, data):\n",
    "        np = self.__NP\n",
    "        cats = 10\n",
    "        catsa = data.__len__() / 4\n",
    "        cats = int(min([cats, catsa]))\n",
    "        n, bins = np.histogram(data, bins=cats, density=True)\n",
    "        m = max(n)\n",
    "        bins = list(bins[1:])\n",
    "        # print(n)\n",
    "        # print(bins)\n",
    "        # print(data)\n",
    "        # print(\"bins=%s, np_max=%s\" % (cats, m))\n",
    "        found = [self._mode_np_helper(x, y, m) for x, y in zip(n, bins)]\n",
    "        found = [x for x in found if x]\n",
    "        # print(found)\n",
    "        if found.__len__() > 0:\n",
    "            max_found = max(found)\n",
    "            # print(\"ve_max=%s\" % max_found)\n",
    "            # print()\n",
    "            return max_found\n",
    "        else:\n",
    "            # print('ve_max not found')\n",
    "            # print()\n",
    "            return None\n",
    "\n",
    "    def _mode(self, data):\n",
    "        \"\"\" Multi library mode.\n",
    "            INPUT: an single array\n",
    "            OUTPUT: float - the most often recurring value\n",
    "        \"\"\"\n",
    "        if self._has_accel_lib(lib='pytorch') and self._is_a_list_of_series(data):\n",
    "            pt = self.__PT\n",
    "            return [float(pt.mode(pt.tensor([*x]))[0]) for x in zip(*data)]\n",
    "\n",
    "        elif self._has_accel_lib(lib='numpy') and self._is_a_list_of_series(data):\n",
    "            return [self._mode_np([*x]) for x in zip(*data)]\n",
    "\n",
    "        raise RuntimeError(\n",
    "            'Accellerator library, numpy or pytorch, requried. Input must be series.'\n",
    "        )\n",
    "\n",
    "    def __round_series(self, x, round_size):\n",
    "        if x != 0 and not x is None:\n",
    "            return round(float(x), round_size)\n",
    "        elif x == 0:\n",
    "            return 0\n",
    "        return None\n",
    "\n",
    "    def round_series(self, data, round_size):\n",
    "        return [self.__round_series(x, round_size) for x in list(data)]\n",
    "\n",
    "    def final_func(self):\n",
    "        \"\"\" Overrride Me. Mandatory.\n",
    "        \"\"\"\n",
    "        raise RuntimeError('Override function required.')\n",
    "\n",
    "    def ignite(self, data, final_func, **kwargs):\n",
    "        if type(data) == tuple:\n",
    "            data = list(data)\n",
    "        round_size = None\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            round_size = kwargs['round_size']\n",
    "        massaged_components = []\n",
    "        for k in kwargs.keys():\n",
    "            if k == 'row_funcs':\n",
    "                for f in kwargs[k]:\n",
    "                    massaged_components.append([f(x) for x in data])\n",
    "            if k == 'columns':\n",
    "                massaged_components.extend(kwargs[k])\n",
    "        if massaged_components.__len__() == 0:\n",
    "            massaged_components = data\n",
    "        result = final_func(massaged_components)\n",
    "        if round_size:\n",
    "            if self._is_series(result):\n",
    "                result = self.round_series(result, round_size)\n",
    "            elif self._is_matrices(result):\n",
    "                new_matrix = []\n",
    "                for row in result:\n",
    "                    new_matrix.append(self.round_series(row, round_size))\n",
    "                result = new_matrix\n",
    "            else:\n",
    "                raise ValueError('result is not a series or a matrix')\n",
    "        if self._is_series(result) and not self.__DROPNA:\n",
    "            # Exploratory result clean ups\n",
    "            if data.__len__() > 0 and self._is_matrices(data[0]) and type(data[0][0][0]) == list:\n",
    "                result = self._square_right_by_ref(data[0][0], result)\n",
    "            elif data.__len__() > 0 and self._is_matrices(data):\n",
    "                result = self._square_right_by_ref(data[0], result)\n",
    "            result = self._square_right_by_ref(data, result)\n",
    "        # Raw output for component mode\n",
    "        return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71c86848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 7, 9, 12, 14, 15, 15, 16, 18, 21, 20, 17, 12, 11, 12, 12, 10, 6)\n",
      "19 19\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "#import torch as pt\n",
    "import numpy as np\n",
    "class VRollingSum(VectorizeEngine):\n",
    "    \"\"\"Parameterized rolling sum as a vectorized algorithm.\n",
    "       6.3 times faster than pandas. Slower than hardcoded vectorization.\n",
    "       PERFORMANCE -\n",
    "           121 µs ± 1.55 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "\n",
    "       BENCHMARK - \n",
    "           Pandas\n",
    "           773 µs ± 42.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__DATA = data\n",
    "        self.__WINDOW = window\n",
    "        chunk1 = ''\n",
    "        for x in range(0, self.__WINDOW):\n",
    "            chunk1 += 'x%s, ' % x\n",
    "        self.__C1 = chunk1[0:-2]\n",
    "        chunk2 = ''\n",
    "        for x in range(0, self.__WINDOW):\n",
    "            chunk2 += 'x%s + ' % x\n",
    "        self.__C2 = chunk2[0:-2]\n",
    "        chunk3 = ''\n",
    "        for x in range(0, self.__WINDOW):\n",
    "            chunk3 += 'data[%s], ' % x\n",
    "        self.__C3 = chunk3[0:-2]\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            return self._add(data)\n",
    "        else:\n",
    "            cmd1 = '[%s for %s in zip(%s)]' % (\n",
    "                 self.__C2, self.__C1, self.__C3\n",
    "            )\n",
    "            rtn = eval(cmd1)\n",
    "            return rtn\n",
    "\n",
    "    def get(self):\n",
    "        data = self.__DATA\n",
    "        x = [data.copy()]\n",
    "        chunk3 = ''\n",
    "        for i in range(1, self.__WINDOW):\n",
    "            chunk3 += 'x.append(self._shift(x[0], %s)) \\n' % i\n",
    "        c3 = chunk3[0:-2]\n",
    "        cmd1 = c3\n",
    "        cmd2 = '%s = self._cropna(x)' % self.__C1\n",
    "        cmd3 = 'self.ignite(data, self.final_func, columns=[%s], row_funcs=[])' % (\n",
    "            self.__C1,\n",
    "        )\n",
    "        exec(cmd1)\n",
    "        exec(cmd2)\n",
    "        rtn = eval(cmd3)\n",
    "        return rtn\n",
    "\n",
    "vectorized_version = VRollingSum(data, 3).get()\n",
    "print(vectorized_version)\n",
    "print(data.__len__(), vectorized_version.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f93b3625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingSum(data, 3, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3be1463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 µs ± 37.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum(data, 3).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b83783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 7.0, 9.0, 12.0, 14.0, 15.0, 15.0, 16.0, 18.0, 21.0, 20.0, 17.0, 12.0, 11.0, 12.0, 12.0, 10.0, 6.0)\n"
     ]
    }
   ],
   "source": [
    "class VRollingSum3(VectorizeEngine):\n",
    "    \"\"\"Hardcoded rolling sum as a vectorized algorithm.\n",
    "       80.3 times faster than pandas. Fastest.\n",
    "       PERFORMANCE -\n",
    "           9.63 µs ± 74.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
    "\n",
    "       BENCHMARK -\n",
    "           Pandas\n",
    "           773 µs ± 42.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - dupilcate a single row vector array and shift them up to three times\n",
    "                   - sum all vector arrays to a reduced result\n",
    "               - Component Engine\n",
    "                   - for each pre prepared vector array\n",
    "                       - sum all vector arrays to a reduced result\n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__DATA = data\n",
    "        self.__ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self.__ROUND_SIZE = kwargs['round_size']\n",
    "        self.__SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self.__SKIP_POST_CLEANING = True\n",
    "            self.__ROUND_SIZE = None\n",
    "        self.__SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self.__SKIP_PRE_PREPARATIONS = True\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            return self._add(data)\n",
    "        else:\n",
    "            return [\n",
    "                x0 + x1 + x2 for x0, x1, x2 in zip(data[0], data[1], data[2])\n",
    "            ]\n",
    "\n",
    "    def get(self):\n",
    "        data = self.__DATA\n",
    "        if self.__SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != 3 and not self._is_matrices(data):\n",
    "                raise ValueError('data array must contain exactly 3 arrays')\n",
    "            x = data.copy()\n",
    "            x0, x1, x2 = x[0], x[1], x[2]\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list type, and contain either a float or int'\n",
    "                )\n",
    "            x0 = data.copy()\n",
    "            x1 = self._shift(x0, 1)\n",
    "            x2 = self._shift(x0, 2)\n",
    "        x0, x1, x2 = self._cropna([x0, x1, x2])\n",
    "        return self.ignite(data, self.final_func, columns=[x0, x1, x2], row_funcs=[], round_size=self.__ROUND_SIZE)\n",
    "\n",
    "vectorized_version = VRollingSum3(data, skip_post_cleaning=False, skip_pre_preparations=False).get()\n",
    "print(vectorized_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c6f6365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 7.0, 9.0, 12.0, 14.0, 15.0, 15.0, 16.0, 18.0, 21.0, 20.0, 17.0, 12.0, 11.0, 12.0, 12.0, 10.0, 6.0)\n",
      "(None, None, 7.0, 9.0, 12.0, 14.0, 15.0, 15.0, 16.0, 18.0, 21.0, 20.0, 17.0, 12.0, 11.0, 12.0, 12.0, 10.0, 6.0)\n"
     ]
    }
   ],
   "source": [
    "v1 = VRollingSum3(data).get()\n",
    "#v2 = VRollingSum3(data, use_pytorch_lib=pt).get()\n",
    "#v3 = VRollingSum3(data, use_tensorflow_lib=tf).get()\n",
    "v4 = VRollingSum3(data, use_numpy_lib=np).get()\n",
    "print(v1)\n",
    "#print(v2)\n",
    "#print(v3)\n",
    "print(v4)\n",
    "#print(v1 == v2 == v3 == v4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2e2b9",
   "metadata": {},
   "source": [
    "## XSmall Array Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67b09806",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x for x in range(0,19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d282055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "418b60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd46f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.6 µs ± 4.19 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7fd551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.6 µs ± 2.63 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f27b0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VectorizeEngine()\n",
    "x0 = data.copy()\n",
    "x1 = v._shift(x0, 1)\n",
    "x2 = v._shift(x0, 2)\n",
    "x0, x1, x2 = v._cropna([x0, x1, x2])\n",
    "data_preprepared = [x0, x1, x2]\n",
    "component_engine_on = {'skip_post_cleaning': True, 'skip_pre_preparations': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5704a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.2 µs ± 1.92 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data_preprepared, use_numpy_lib=np, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec2f839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2 µs ± 279 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data_preprepared, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea6f73",
   "metadata": {},
   "source": [
    "## Small Array Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "060deed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x for x in range(0,1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a845807d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdab87a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3d624c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 ms ± 36.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43899a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840 µs ± 7.88 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acacce3",
   "metadata": {},
   "source": [
    "## Medium Array Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bce4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x for x in range(0,100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41531b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69d4030a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea895219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 ms ± 7.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c50e2eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 ms ± 4.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac12b9d",
   "metadata": {},
   "source": [
    "## Large Array Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23011362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x for x in range(1,12500000)]  # 12.5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "632e8cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a539ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit VRollingSum3(data, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e196494d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 s ± 719 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "timeit VRollingSum3(data, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingSum3(data).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VectorizeEngine()\n",
    "x0 = data.copy()\n",
    "x1 = v._shift(x0, 1)\n",
    "x2 = v._shift(x0, 2)\n",
    "x0, x1, x2 = v._cropna([x0, x1, x2])\n",
    "data_preprepared = [x0, x1, x2]\n",
    "component_engine_on = {'skip_post_cleaning': True, 'skip_pre_preparations': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f776bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingSum3(data_preprepared, use_numpy_lib=np, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d80e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingSum3(data_preprepared, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7151d6",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78ae1d",
   "metadata": {},
   "source": [
    "Tensorflow and PyTorch did not add any value when trying to speed up mathematical operations with large vector arrays against each other. <br/>\n",
    "\n",
    "Using numpy is quicker than gpu libraries, but still slower than pure functional python with generators and iterators. <br/>\n",
    "\n",
    "We can conlude, that if our data set type is that of single row vector arrays being computed against other single row arrays, then the most performant method will always be a minimal algorithm with pure functional python primitive operations.<br/>\n",
    "\n",
    "We should see futher if the value of gpu libraries like Tensorflow and PyTorch, shine above everyone, when used for their matrix operations where we have multi-row tensor arrays computed against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26496234",
   "metadata": {},
   "source": [
    "# Multiplication, Subtraction and Division Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bed69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data1 = [\n",
    "    [random.randint(1, 100) / 100 for x in range(0, 20)],\n",
    "    [random.randint(1, 100) / 100 for x in range(0, 20)],\n",
    "    [random.randint(1, 100) / 100 for x in range(0, 20)]\n",
    "]\n",
    "data2 = [\n",
    "    [1 for x in range(1, 20)],\n",
    "    [2 for x in range(1, 20)],\n",
    "    [3 for x in range(1, 20)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ec750",
   "metadata": {},
   "source": [
    "## Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b699080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingProd3(VectorizeEngine):\n",
    "    \"\"\"Hardcoded rolling prod as a vectorized algorithm.\n",
    "       Useless example. Only for testing and validation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__DATA = data\n",
    "        self.__ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self.__ROUND_SIZE = kwargs['round_size']\n",
    "        self.__SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self.__SKIP_POST_CLEANING = True\n",
    "            self.__ROUND_SIZE = None\n",
    "        self.__SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self.__SKIP_PRE_PREPARATIONS = True\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            return self._multiply(data)\n",
    "        else:\n",
    "            return [\n",
    "                x0 * x1 * x2 for x0, x1, x2 in zip(data[0], data[1], data[2])\n",
    "            ]\n",
    "\n",
    "    def get(self):\n",
    "        data = self.__DATA\n",
    "        if data.__len__() != 3 and not self._is_matrices(data):\n",
    "            raise ValueError('data array must contain exactly 3 arrays')\n",
    "        x = data.copy()\n",
    "        x0, x1, x2 = x[0], x[1], x[2]\n",
    "        x0, x1, x2 = self._cropna([x0, x1, x2])\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[x0, x1, x2], row_funcs=[], round_size=self.__ROUND_SIZE\n",
    "        )\n",
    "\n",
    "vectorized_version = VRollingProd3(data1).get()\n",
    "print(vectorized_version)\n",
    "vectorized_version = VRollingProd3(data2).get()\n",
    "print(vectorized_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e894b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = VRollingProd3(data1).get()\n",
    "#v2 = VRollingProd3(data1, use_pytorch_lib=pt).get()\n",
    "#v3 = VRollingProd3(data1, use_tensorflow_lib=tf).get()\n",
    "v4 = VRollingProd3(data1, use_numpy_lib=np).get()\n",
    "print(v1)\n",
    "#print(v2)\n",
    "#print(v3)\n",
    "print(v4)\n",
    "#print(v1 == v2 == v3 == v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd244124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit VRollingProd3(data1, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3919f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingProd3(data1, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingProd3(data1, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b451da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit VRollingProd3(data1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprepared = data1\n",
    "component_engine_on = {'skip_post_cleaning': True, 'skip_pre_preparations': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33877b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingProd3(data_preprepared, use_numpy_lib=np, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingProd3(data_preprepared, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d5aeb",
   "metadata": {},
   "source": [
    "## Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingSub3(VectorizeEngine):\n",
    "    \"\"\"Hardcoded rolling sub as a vectorized algorithm.\n",
    "       Useless example. Only for testing and validation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__DATA = data\n",
    "        self.__ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self.__ROUND_SIZE = kwargs['round_size']\n",
    "        self.__SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self.__SKIP_POST_CLEANING = True\n",
    "            self.__ROUND_SIZE = None\n",
    "        self.__SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self.__SKIP_PRE_PREPARATIONS = True\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            return self._subtract(data)\n",
    "        else:\n",
    "            return [\n",
    "                x0 - x1 - x2 for x0, x1, x2 in zip(data[0], data[1], data[2])\n",
    "            ]\n",
    "\n",
    "    def get(self):\n",
    "        data = self.__DATA\n",
    "        if data.__len__() != 3 and not self._is_matrices(data):\n",
    "            raise ValueError('data array must contain exactly 3 arrays')\n",
    "        x = data.copy()\n",
    "        x0, x1, x2 = x[0], x[1], x[2]\n",
    "        x0, x1, x2 = self._cropna([x0, x1, x2])\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[x0, x1, x2], row_funcs=[], round_size=self.__ROUND_SIZE\n",
    "        )\n",
    "\n",
    "vectorized_version = VRollingSub3(data1).get()\n",
    "print(vectorized_version)\n",
    "vectorized_version = VRollingSub3(data2).get()\n",
    "print(vectorized_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = VRollingSub3(data1).get()\n",
    "#v2 = VRollingSub3(data1, use_pytorch_lib=pt).get()\n",
    "#v3 = VRollingSub3(data1, use_tensorflow_lib=tf).get()\n",
    "v4 = VRollingSub3(data1, use_numpy_lib=np).get()\n",
    "print(v1)\n",
    "#print(v2)\n",
    "#print(v3)\n",
    "print(v4)\n",
    "#print(v1 == v2 == v3 == v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit VRollingSub3(data1, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299232ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingSub3(data1, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ed0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingSub3(data1, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff7bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit VRollingSub3(data1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprepared = data1\n",
    "component_engine_on = {'skip_post_cleaning': True, 'skip_pre_preparations': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80014c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingSub3(data_preprepared, use_numpy_lib=np, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087013da",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingSub3(data_preprepared, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3c53d",
   "metadata": {},
   "source": [
    "## Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingDiv3(VectorizeEngine):\n",
    "    \"\"\"Hardcoded rolling divide as a vectorized algorithm.\n",
    "       Useless example. Only for testing and validation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__DATA = data\n",
    "        self.__ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self.__ROUND_SIZE = kwargs['round_size']\n",
    "        self.__SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self.__SKIP_POST_CLEANING = True\n",
    "            self.__ROUND_SIZE = None\n",
    "        self.__SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self.__SKIP_PRE_PREPARATIONS = True\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            return self._divide(data)\n",
    "        else:\n",
    "            return [\n",
    "                x0 / x1 / x2 for x0, x1, x2 in zip(data[0], data[1], data[2])\n",
    "            ]\n",
    "\n",
    "    def get(self):\n",
    "        data = self.__DATA\n",
    "        if data.__len__() != 3 and not self._is_matrices(data):\n",
    "            raise ValueError('data array must contain exactly 3 arrays')\n",
    "        x = data.copy()\n",
    "        x0, x1, x2 = x[0], x[1], x[2]\n",
    "        # x0, x1, x2 = self._cropna([x0, x1, x2])\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[x0, x1, x2], row_funcs=[], round_size=self.__ROUND_SIZE\n",
    "        )\n",
    "\n",
    "vectorized_version = VRollingDiv3(data1).get()\n",
    "print(vectorized_version)\n",
    "vectorized_version = VRollingDiv3(data2).get()\n",
    "print(vectorized_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356aab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = VRollingDiv3(data1).get()\n",
    "#v2 = VRollingDiv3(data1, use_pytorch_lib=pt).get()\n",
    "#v3 = VRollingDiv3(data1, use_tensorflow_lib=tf).get()\n",
    "v4 = VRollingDiv3(data1, use_numpy_lib=np).get()\n",
    "print(v1)\n",
    "#print(v2)\n",
    "#print(v3)\n",
    "print(v4)\n",
    "#print(v1 == v2 == v3 == v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aacf7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit VRollingDiv3(data1, use_tensorflow_lib=tf).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d6ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingDiv3(data1, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingDiv3(data1, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a7697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit VRollingDiv3(data1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprepared = data1\n",
    "component_engine_on = {'skip_post_cleaning': True, 'skip_pre_preparations': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingDiv3(data_preprepared, use_numpy_lib=np, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cccd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingDiv3(data_preprepared, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676f215",
   "metadata": {},
   "source": [
    "## Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1339238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingPower3(VectorizeEngine):\n",
    "    \"\"\"Hardcoded rolling divide as a vectorized algorithm.\n",
    "       Useless example. Only for testing and validation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__DATA = data\n",
    "        self.__ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self.__ROUND_SIZE = kwargs['round_size']\n",
    "        self.__SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self.__SKIP_POST_CLEANING = True\n",
    "            self.__ROUND_SIZE = None\n",
    "        self.__SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self.__SKIP_PRE_PREPARATIONS = True\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            return self._add([\n",
    "                self._power([data[0], data[1]]), data[2]\n",
    "            ])\n",
    "        else:\n",
    "            return [\n",
    "                x0**x1+x2 for x0, x1, x2 in zip(data[0], data[1], data[2])\n",
    "            ]\n",
    "\n",
    "    def get(self):\n",
    "        data = self.__DATA\n",
    "        if data.__len__() != 3 and not self._is_matrices(data):\n",
    "            raise ValueError('data array must contain exactly 3 arrays')\n",
    "        x = data.copy()\n",
    "        x0, x1, x2 = x[0], x[1], x[2]\n",
    "        # x0, x1, x2 = self._cropna([x0, x1, x2])\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[x0, x1, x2], row_funcs=[], round_size=self.__ROUND_SIZE\n",
    "        )\n",
    "\n",
    "vectorized_version = VRollingPower3(data1).get()\n",
    "print(vectorized_version[:5])\n",
    "vectorized_version = VRollingPower3(data2).get()\n",
    "print(vectorized_version[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72337d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = VRollingPower3(data1).get()[-100:10]\n",
    "#v2 = VRollingPower3(data1, use_pytorch_lib=pt).get()[-100:10]\n",
    "# v3 = VRollingPower3(data1, use_tensorflow_lib=tf).get()[:50]\n",
    "v4 = VRollingPower3(data1, use_numpy_lib=np).get()[-100:10]\n",
    "print(v1[-10:])\n",
    "#print(v2[-10:])\n",
    "# print(v3[-10:])\n",
    "print(v4[-10:]) \n",
    "#print(v1 == v2 == v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aeb1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit VRollingPower3(data1, use_pytorch_lib=pt).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ab37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingPower3(data1, use_numpy_lib=np).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a070b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit VRollingPower3(data1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616938b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprepared = data1\n",
    "component_engine_on = {'skip_post_cleaning': True, 'skip_pre_preparations': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VRollingPower3(data_preprepared, use_numpy_lib=np, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7676ad",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# Vectorized Stats Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6423601",
   "metadata": {},
   "source": [
    "We will build a set of tools to use for orchestrating a composite design, that aids in forecasting and performance comparison benchmarking.\n",
    "\n",
    "We value hardcoding capabilities over parameterization if it means more speed.\n",
    "\n",
    "We value numpy or pure python for single array computations.\n",
    "\n",
    "We value gpu pytorch gpu library for multi row array matrix computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data = [round(random.randint(1, 1000) / 1000, 6) for x in range(0, 1000)]\n",
    "datax = [round(random.randint(1, 1000) / 1000, 6) for x in range(0, 1000)]\n",
    "data_df = pd.DataFrame(data)\n",
    "datac = [\n",
    "    data[6:],\n",
    "    [round(float(x), 6) for x in data_df.shift(1).values][6:],\n",
    "    [round(float(x), 6) for x in data_df.shift(2).values][6:],\n",
    "    [round(float(x), 6) for x in data_df.shift(3).values][6:],\n",
    "    [round(float(x), 6) for x in data_df.shift(4).values][6:],\n",
    "    [round(float(x), 6) for x in data_df.shift(5).values][6:],\n",
    "    [round(float(x), 6) for x in data_df.shift(6).values][6:]\n",
    "]\n",
    "data1 = [\n",
    "    [random.randint(1, 100) / 100 for x in range(0, 1000)],\n",
    "    [random.randint(1, 100) / 100 for x in range(0, 1000)],\n",
    "    [random.randint(1, 100) / 100 for x in range(0, 1000)]\n",
    "]\n",
    "data2 = [\n",
    "    [1 for x in range(0, 1000)],\n",
    "    [2 for x in range(0, 1000)],\n",
    "    [3 for x in range(0, 1000)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a29b1",
   "metadata": {},
   "source": [
    "### Common Rolling Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd61764",
   "metadata": {},
   "source": [
    "Mean, Standard Deviation, Min, Max, Median, Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca50b7",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingSumEngine(VectorizeEngine):\n",
    "    \"\"\"Rolling sum as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - dupilcate a single row vector array and shift them up to x times\n",
    "                   - sum all vector arrays to a reduced result\n",
    "               - Component Engine\n",
    "                   - for each pre prepared vector array\n",
    "                       - sum all vector arrays to a reduced result\n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__WINDOW = window\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        f = lambda *args: sum(*args)\n",
    "        return [\n",
    "            f(x) for x in zip(*data)\n",
    "        ]\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            return self._add(data)\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != self.__WINDOW or not self._is_matrices(data):\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly %s arrays' % self.__WINDOW\n",
    "                )\n",
    "            x = tuple(data.copy())\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list type, ' +\n",
    "                    'and contain either a float or int'\n",
    "                )\n",
    "            x = self._vshift(data.copy(), 0, self.__WINDOW)\n",
    "        x = self._cropna(list(x))\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=list(x), row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum (data, window, **kwargs):\n",
    "    return VRollingSumEngine(data, window, **kwargs).get()\n",
    "\n",
    "def sum3(data, **kwargs):\n",
    "    return VRollingSumEngine(data, 3, **kwargs).get()\n",
    "\n",
    "def sum7(data, **kwargs):\n",
    "    return VRollingSumEngine(data, 7, **kwargs).get()\n",
    "\n",
    "def sum14(data, **kwargs):\n",
    "    return VRollingSumEngine(data, 14, **kwargs).get()\n",
    "\n",
    "def sum32(data, **kwargs):\n",
    "    return VRollingSumEngine(data, 32, **kwargs).get()\n",
    "\n",
    "def sum64(data, **kwargs):\n",
    "    return VRollingSumEngine(data, 64, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.__len__(), sum3(data).__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum7(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26082de",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum7(data, use_numpy_lib=np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit sum7(data, use_pytorch_lib=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c275d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit sum7(data, use_tensorflow_lib=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06eb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum14(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c53c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum14(data, use_numpy_lib=np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit sum14(data, use_pytorch_lib=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15453762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timeit sum14(data, use_tensorflow_lib=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ea882",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum32(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum32(data, use_numpy_lib=np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit sum32(data, use_pytorch_lib=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e67e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit sum32(data, use_tensorflow_lib=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c987f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum64(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03683461",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum64(data, use_numpy_lib=np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ac588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit sum64(data, use_pytorch_lib=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a98dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit sum64(data, use_tensorflow_lib=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingAvgEngine(VectorizeEngine):\n",
    "    \"\"\"Rolling avg as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - use a rolling sum engine to perform a configurable computation\n",
    "                   - use this engine to perform a pure python vectorized rolling average\n",
    "               - Component Engine\n",
    "                   - the same as the final engine, except -\n",
    "                       - you must pass in pre prepared colum series for the rolling sum computation\n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__VRSUM = VRollingSumEngine(data, window, **kwargs).get\n",
    "        self.__AVG_WINDOW = window\n",
    "\n",
    "    def _avg(self, vrsum, divisor):\n",
    "        f = lambda a, b: a / b\n",
    "        return tuple(\n",
    "            [\n",
    "                f(rsum, total) for rsum, total in zip(\n",
    "                    vrsum, divisor\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        divisor = [self.__AVG_WINDOW for x in range(0, data.__len__())]\n",
    "        return self._avg(data, divisor)\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            raise RuntimeError('Average component must be pure python.')\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = self.__VRSUM()\n",
    "        data = [x for x in data if x]\n",
    "        if not self._SKIP_POST_CLEANING:\n",
    "            return self._square_right_by_ref(\n",
    "                self._DATA,\n",
    "                self.ignite(\n",
    "                    data, self.final_func, columns=[], row_funcs=[],\n",
    "                    round_size=self._ROUND_SIZE\n",
    "                )\n",
    "            )\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[], row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc991c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg3(data, **kwargs):\n",
    "    return VRollingAvgEngine(data, 3, **kwargs).get()\n",
    "\n",
    "def avg7(data, **kwargs):\n",
    "    return VRollingAvgEngine(data, 7, **kwargs).get()\n",
    "\n",
    "def avg14(data, **kwargs):\n",
    "    return VRollingAvgEngine(data, 14, **kwargs).get()\n",
    "\n",
    "def avg32(data, **kwargs):\n",
    "    return VRollingAvgEngine(data, 32, **kwargs).get()\n",
    "\n",
    "def avg64(data, **kwargs):\n",
    "    return VRollingAvgEngine(data, 64, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.__len__(), avg3(data).__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit avg3(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit avg7(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit avg14(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e14425",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit avg32(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cdfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit avg64(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VectorizeEngine()\n",
    "smas = V._square_right([avg14(data), avg7(data)])\n",
    "pd.DataFrame({'a': smas[0], 'b': smas[1]}).tail(20).plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg7(data).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg64(data).__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7246ad1f",
   "metadata": {},
   "source": [
    "#### Standard Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingReturnEngine(VectorizeEngine):\n",
    "    \"\"\" Rolling return as a vectorized algorithm.\n",
    "        Must be raw values that are all positive. No +/- rate of change values!\n",
    "\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - \n",
    "               - Component Engine\n",
    "                   - \n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "                     window - int - rolling period size\n",
    "                     uom - D,M,Q,Y - unit of measure - daily, monthly, quarterly, yearly\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "                     window - int - rolling period size\n",
    "                     uom - D,M,Q,Y - unit of measure - daily, monthly, quarterly, yearly\n",
    "                     geo - ANN, MON, QTR - geometric basis -\n",
    "                             - annualization, quarterlization, monthlyization\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, uom, geo, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__WINDOW = window\n",
    "        self.__UNIT_OF_MEASURE = None\n",
    "        if uom == 'D' and geo == 'ANN':\n",
    "            self.__UNIT_OF_MEASURE = 365\n",
    "        elif uom == 'M' and geo == 'ANN':\n",
    "            self.__UNIT_OF_MEASURE = 12\n",
    "        elif uom == 'Q' and geo == 'ANN':\n",
    "            self.__UNIT_OF_MEASURE = 4\n",
    "        elif uom == 'Y' and geo == 'ANN':\n",
    "            self.__UNIT_OF_MEASURE = 1\n",
    "        elif uom == 'D' and geo == 'MON':\n",
    "            self.__UNIT_OF_MEASURE = 31\n",
    "        elif uom == 'M' and geo == 'MON':\n",
    "            self.__UNIT_OF_MEASURE = 1\n",
    "        elif uom == 'D' and geo == 'QTR':\n",
    "            self.__UNIT_OF_MEASURE = 93\n",
    "        elif uom == 'M' and geo == 'QTR':\n",
    "            self.__UNIT_OF_MEASURE = 3\n",
    "        elif uom == 'Q' and geo == 'QTR':\n",
    "            self.__UNIT_OF_MEASURE = 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'uom must be either D,M,Q, or Y, with geo basis as either ANN, MON or QTR'\n",
    "            )\n",
    "\n",
    "    def __pure_python_func_helper(self, *data):\n",
    "        C = data[0][0]\n",
    "        P = data[0][-1]\n",
    "        S = C - P\n",
    "        if S == 0:\n",
    "            return C\n",
    "        R = S / P\n",
    "        SR = abs(R)**(float(self.__UNIT_OF_MEASURE) / self.__WINDOW)\n",
    "        if R < 0:\n",
    "            SR = SR * -1\n",
    "        return SR\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        return [self.__pure_python_func_helper(x) for x in zip(*data)]\n",
    "\n",
    "    def __accel_func_helper(self, R, STD):\n",
    "        SR = R**STD\n",
    "        if R < 0:\n",
    "            SR = SR * -1\n",
    "        return SR\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            P = data[-1]\n",
    "            C = data[0]\n",
    "            uoms = [self.__UNIT_OF_MEASURE for x in range(0, data[0].__len__())]\n",
    "            hold = [self.__WINDOW for x in range(0, data[0].__len__())]\n",
    "            R = self._divide([self._subtract([C, P]), P])\n",
    "            STD = self._divide([uoms, hold])\n",
    "            return [self.__accel_func_helper(rtn, std) for rtn, std in zip(R, STD)]\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != self.__WINDOW or not self._is_matrices(data):\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly %s arrays' % self.__WINDOW\n",
    "                )\n",
    "            x = tuple(data.copy())\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list ' +\n",
    "                    'type, and contain either a float or int'\n",
    "                )\n",
    "            x = self._vshift(data.copy(), 0, self.__WINDOW)\n",
    "        x = self._cropna(list(x))\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=list(x), row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtn(data, window, uom, geo, **kwargs):\n",
    "    return VRollingReturnEngine(data, window, uom, geo, **kwargs).get()\n",
    "\n",
    "def rtn3(data, uom, geo, **kwargs):\n",
    "    return VRollingReturnEngine(data, 3, uom, geo, **kwargs).get()\n",
    "\n",
    "def rtn7(data, uom, geo, **kwargs):\n",
    "    return VRollingReturnEngine(data, 7, uom, geo, **kwargs).get()\n",
    "\n",
    "def rtn14(data, uom, geo, **kwargs):\n",
    "    return VRollingReturnEngine(data, 14, uom, geo, **kwargs).get()\n",
    "\n",
    "def rtn32(data, uom, geo, **kwargs):\n",
    "    return VRollingReturnEngine(data, 32, uom, geo, **kwargs).get()\n",
    "\n",
    "def rtn64(data, uom, geo, **kwargs):\n",
    "    return VRollingReturnEngine(data, 64, uom, geo, **kwargs).get()\n",
    "\n",
    "def rtn_8M_D_MON(data, **kwargs):\n",
    "    # 8 month hold(248p), daily precision, average monthly realized returns, geometrically normalised\n",
    "    return rtn(data, 31 * 8, 'D', 'MON', **kwargs)\n",
    "\n",
    "def rtn_4M_D_MON(data, **kwargs):\n",
    "    # 4 month hold(124p), daily precision, average monthly realized returns, geometrically normalised\n",
    "    return rtn(data, 31 * 4, 'D', 'MON', **kwargs)\n",
    "\n",
    "def rtn_2M_D_MON(data, **kwargs):\n",
    "    # 2 month hold(62p), daily precision, average monthly realized returns, geometrically normalised\n",
    "    return rtn(data, 31 * 2, 'D', 'MON', **kwargs)\n",
    "\n",
    "def rtn_2Q_D_QTR(data, **kwargs):\n",
    "    # 2 quarter hold(186p), daily precision, average quarterly realized returns\n",
    "    return rtn(data, 31 * 3 * 2, 'D', 'QTR', **kwargs)\n",
    "\n",
    "def rtn_2Y_D_ANN(data, **kwargs):\n",
    "    # 2 year hold(744p), daily precision, average annual realized returns\n",
    "    return rtn(data, 31 * 12 * 2, 'D', 'ANN', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = [float(x / 100) for x in range(1,6)]\n",
    "xdata.extend([float((x / 100)) for x in range(6, 1, -1)])\n",
    "print(xdata[3:])\n",
    "result = rtn3(xdata, 'M', 'MON')[3:]\n",
    "print(result)\n",
    "print(xdata.__len__(), result.__len__())\n",
    "ut.TestCase().assertTrue(result[0] > result[-1] and result[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = [float((x / 100)) for x in range(64, 1, -1)]\n",
    "print(xdata[-5:])\n",
    "result = rtn32(xdata, 'M', 'MON')[-5:]\n",
    "print(result)\n",
    "ut.TestCase().assertTrue(result[0] > result[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a52fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn_2M_D_MON(data)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn_4M_D_MON(data)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn_8M_D_MON(data)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf54568",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn_2Q_D_QTR(data)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce33b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn_2Y_D_ANN(data)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77062af",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = tuple([x for x in rtn7(data, 'M', 'MON', dropna=True)])\n",
    "result2 = tuple([round(float(x), 6) for x in rtn7(datac, 'M', 'MON', **component_engine_on)])\n",
    "print(result1[:20])\n",
    "print(result2[:20])\n",
    "ut.TestCase().assertTrue(result1 == result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b777d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result2[:40]).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009e438",
   "metadata": {},
   "source": [
    "#### Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5845e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingMinEngine(VectorizeEngine):\n",
    "    \"\"\" Rolling min as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - \n",
    "               - Component Engine\n",
    "                   - \n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__WINDOW = window\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        f = lambda *args: min(*args)\n",
    "        return [\n",
    "            f(x) for x in zip(*data)\n",
    "        ]\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            raise RuntimeError('Min component must be pure python.')\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != self.__WINDOW or not self._is_matrices(data):\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly %s arrays' % self.__WINDOW\n",
    "                )\n",
    "            x = tuple(data.copy())\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list ' +\n",
    "                    'type, and contain either a float or int'\n",
    "                )\n",
    "            x = self._vshift(data.copy(), 0, self.__WINDOW)\n",
    "        x = self._cropna(list(x))\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=list(x), row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min3(data, **kwargs):\n",
    "    return VRollingMinEngine(data, 3, **kwargs).get()\n",
    "\n",
    "def min7(data, **kwargs):\n",
    "    return VRollingMinEngine(data, 7, **kwargs).get()\n",
    "\n",
    "def min14(data, **kwargs):\n",
    "    return VRollingMinEngine(data, 14, **kwargs).get()\n",
    "\n",
    "def min32(data, **kwargs):\n",
    "    return VRollingMinEngine(data, 32, **kwargs).get()\n",
    "\n",
    "def min64(data, **kwargs):\n",
    "    return VRollingMinEngine(data, 64, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7454f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = min3(data)\n",
    "print(data.__len__(), result.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit min3(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit min7(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15142ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit min14(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b10828",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit min32(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb050f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit min64(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f757392",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = min7(data, dropna=True)\n",
    "b = tuple([round(x, 3) for x in min7(datac, **component_engine_on)])\n",
    "print(a[-5:])\n",
    "print(b[-5:])\n",
    "ut.TestCase().assertTrue(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379cb83",
   "metadata": {},
   "source": [
    "#### Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingMaxEngine(VectorizeEngine):\n",
    "    \"\"\" Rolling max as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - \n",
    "               - Component Engine\n",
    "                   - \n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__WINDOW = window\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        f = lambda *args: max(*args)\n",
    "        return [\n",
    "            f(x) for x in zip(*data)\n",
    "        ]\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            raise RuntimeError('Max component must be pure python.')\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != self.__WINDOW or not self._is_matrices(data):\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly %s arrays' % self.__WINDOW\n",
    "                )\n",
    "            x = tuple(data.copy())\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list ' +\n",
    "                    'type, and contain either a float or int'\n",
    "                )\n",
    "            x = self._vshift(data.copy(), 0, self.__WINDOW)\n",
    "        x = self._cropna(list(x))\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=list(x), row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max3(data, **kwargs):\n",
    "    return VRollingMaxEngine(data, 3, **kwargs).get()\n",
    "\n",
    "def max7(data, **kwargs):\n",
    "    return VRollingMaxEngine(data, 7, **kwargs).get()\n",
    "\n",
    "def max14(data, **kwargs):\n",
    "    return VRollingMaxEngine(data, 14, **kwargs).get()\n",
    "\n",
    "def max32(data, **kwargs):\n",
    "    return VRollingMaxEngine(data, 32, **kwargs).get()\n",
    "\n",
    "def max64(data, **kwargs):\n",
    "    return VRollingMaxEngine(data, 64, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595d584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit max3(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7cbf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit max7(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit max14(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34f491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit max32(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit max64(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958aae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = max7(data, dropna=True)\n",
    "b = tuple([round(x, 3) for x in max7(datac, **component_engine_on)])\n",
    "print(a[-5:])\n",
    "print(b[-5:])\n",
    "ut.TestCase().assertTrue(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d92b52",
   "metadata": {},
   "source": [
    "#### Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02fcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRollingModeEngine(VectorizeEngine):\n",
    "    \"\"\"Rolling mode as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - use np.histogram or pt.mode functions to determine most often recuring values\n",
    "                   - both libraries will yeild difference results because they are similar in implementation\n",
    "                   - we dot not care about exactness\n",
    "                   - numpy is defaulted to 10 bins if window is equal to or larger than 40\n",
    "                   - pytorch has its own method to yield mode\n",
    "                   - numpy is more reactive to rolling fluctuations\n",
    "               - Component Eine\n",
    "                   - the same as the final engine, except -\n",
    "                       - you must pass in pre prepared colum series for the rolling sum computation\n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 2\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__WINDOW = window\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib(lib='numpy') or self._has_accel_lib(lib='pytorch'):\n",
    "            return self._mode(data)\n",
    "        else:\n",
    "            raise RuntimeError('Mode component must use numpy or pytorch.')\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != self.__WINDOW or not self._is_matrices(data):\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly %s arrays' % self.__WINDOW\n",
    "                )\n",
    "            x = tuple(data.copy())\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list ' +\n",
    "                    'type, and contain either a float or int'\n",
    "                )\n",
    "            x = self._vshift(data.copy(), 0, self.__WINDOW)\n",
    "        x = self._cropna(list(x))\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=list(x), row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9f464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = tuple([round(x, 2) for x in VRollingModeEngine(data, 7, use_numpy_lib=np, dropna=True).get()])\n",
    "b = tuple([round(x, 2) for x in VRollingModeEngine(datac, 7, use_numpy_lib=np, **component_engine_on).get()])\n",
    "print(a[-100:])\n",
    "print(b[-100:])\n",
    "print(a.__len__(), b.__len__())\n",
    "ok = True\n",
    "c = 0\n",
    "v = 0\n",
    "for q, w in zip(a, b):\n",
    "    c += 1\n",
    "    if q != w:\n",
    "        print(c, q, w)\n",
    "        v +=  abs(q - w)\n",
    "        ok = False\n",
    "v = float(v / c)\n",
    "print('+/- variance = %s' % v)\n",
    "ut.TestCase().assertTrue(v < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tuple([round(x, 2) for x in VRollingModeEngine(data, 7, use_pytorch_lib=pt, dropna=True).get()])\n",
    "b = tuple([round(x, 2) for x in VRollingModeEngine(datac, 7, use_pytorch_lib=pt, **component_engine_on).get()])\n",
    "print(a[-100:])\n",
    "print(b[-100:])\n",
    "print(a.__len__(), b.__len__())\n",
    "ok = True\n",
    "c = 0\n",
    "v = 0\n",
    "for q, w in zip(a, b):\n",
    "    c += 1\n",
    "    if q != w:\n",
    "        print(c, q, w)\n",
    "        v +=  abs(q - w)\n",
    "        ok = False\n",
    "v = float(v / c)\n",
    "print('+/- variance = %s' % v)\n",
    "ut.TestCase().assertTrue(v < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b1793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xdata = data.copy()\n",
    "xdata.extend([0.3 for x in range(0,10)])\n",
    "xdata.extend(data[:10])\n",
    "a = tuple([round(float(x), 2) for x in VRollingModeEngine(xdata, 100, use_pytorch_lib=pt, dropna=True).get()])\n",
    "b = tuple([round(float(x), 2) for x in VRollingModeEngine(xdata, 100, use_numpy_lib=np, dropna=True).get()])\n",
    "print(a[-100:])\n",
    "print(b[-100:])\n",
    "print(a.__len__(), b.__len__())\n",
    "ok = True\n",
    "c = 0\n",
    "v = 0\n",
    "for q, w in zip(a, b):\n",
    "    c += 1\n",
    "    if q != w:\n",
    "        print(c, q, w)\n",
    "        v +=  abs(q - w)\n",
    "        ok = False\n",
    "v = float(v / c)\n",
    "print('+/- variance = %s' % v)\n",
    "ut.TestCase().assertTrue(v < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfebb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xdata = [0.727, 0.711, 0.579, 0.779, 0.441, 0.123, 0.428, 0.912, 0.902, 0.515, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.988, 0.983, 0.987, 0.219, 0.108, 0.807, 0.239, 0.938, 0.719, 0.49, 0.789, 0.299, 0.823, 0.06, 0.083, 0.095, 0.762, 0.39, 0.614, 0.572, 0.34, 0.277, 0.917, 0.118, 0.644, 0.211, 0.258, 0.929, 0.923, 0.971, 0.18, 0.545, 0.111, 0.831, 0.833, 0.219, 0.586, 0.217, 0.124, 0.32, 0.665, 0.562, 0.543, 0.287, 0.97, 0.163, 0.858, 0.4, 0.175, 0.384, 0.128, 0.334, 0.44, 0.371, 0.151, 0.742, 0.08, 0.641, 0.325, 0.033, 0.199, 0.556, 0.151, 0.125, 0.848, 0.49, 0.878, 0.605, 0.699, 0.624, 0.556, 0.564, 0.557, 0.591, 0.385, 0.393, 0.523, 0.401, 0.504, 0.788]\n",
    "F = np.histogram(xdata, bins=10, density=False)\n",
    "plt.plot(F[1][1:], F[0])\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a115d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = tuple([round(float(x), 2) for x in VRollingModeEngine(datac, 7, use_pytorch_lib=pt, **component_engine_on).get()])\n",
    "c = tuple([round(float(x), 2) for x in VRollingModeEngine(datac, 7, use_numpy_lib=np, **component_engine_on).get()])\n",
    "print(a[-100:])\n",
    "print(b[-100:])\n",
    "print(a.__len__(), b.__len__())\n",
    "ok = True\n",
    "c = 0\n",
    "v = 0\n",
    "for q, w in zip(a, b):\n",
    "    c += 1\n",
    "    if q != w:\n",
    "        print(c, q, w)\n",
    "        v +=  abs(q - w)\n",
    "        ok = False\n",
    "v = float(v / c)\n",
    "print('+/- variance = %s' % v)\n",
    "ut.TestCase().assertTrue(v < 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a000d",
   "metadata": {},
   "source": [
    "#### Cumulative Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa3b9d",
   "metadata": {},
   "source": [
    "Just use sum3, sum7, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = [\n",
    "    [5, 5, 5],\n",
    "    [-3, -3, -3],\n",
    "    [-2, -2, -2]\n",
    "]\n",
    "sum3(data3, **component_engine_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = [\n",
    "    [1, 5, -2],\n",
    "    [3, 1, 5],\n",
    "    [-1, 3, 1]\n",
    "]\n",
    "sum3(data4, **component_engine_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d93a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = [-1,3,1,5,-2]\n",
    "sum3(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum7(data, dropna=True)\n",
    "b = tuple([round(x, 3) for x in sum7(datac, **component_engine_on)])\n",
    "print(a[-5:])\n",
    "print(b[-5:])\n",
    "ut.TestCase().assertTrue(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66301375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit sum3(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96ac60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit sum7(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ffa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit sum14(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36023466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit sum32(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c747e",
   "metadata": {},
   "source": [
    "### Basic Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d870fc",
   "metadata": {},
   "source": [
    "#### Growth Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6144ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGrowthRateEngine(VectorizeEngine):\n",
    "    \"\"\"Growth rate as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - \n",
    "               - Component Engine\n",
    "                   - \n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__SHIFTS = 2\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        f = lambda *args: (args[0][0] - args[0][1]) / args[0][1]\n",
    "        return [f(x) for x in zip(*data)]\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            c, p = data[0], data[1]\n",
    "            return self._divide(self._subtract(c, p), p)\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != self.__SHIFTS or not self._is_matrices(data):\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly %s arrays' % self.__SHIFTS\n",
    "                )\n",
    "            x = tuple(data.copy())\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list ' +\n",
    "                    'type, and contain either a float or int'\n",
    "                )\n",
    "            x = self._vshift(data.copy(), 0, self.__SHIFTS)\n",
    "            x = self._cropna(list(x))\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=list(x), row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ed617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr(data, **kwargs):\n",
    "    return VGrowthRateEngine(data, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pandas growth rate with vectorized one. Check results.\n",
    "gr_df = [round(x, 6) for x in data_df[0].pct_change().dropna().values]\n",
    "g = [x for x in VGrowthRateEngine(data).get() if not x is None]\n",
    "for a, b in zip(gr_df, g):\n",
    "    if a != b:\n",
    "        print(a, b)\n",
    "gr_df == g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb81fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df[0].count())\n",
    "print(data.__len__())\n",
    "print(data1[0].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b34b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tuple([round(x, 6) for x in VGrowthRateEngine(data[5:], dropna=True).get()])\n",
    "b = tuple([round(x, 6) for x in VGrowthRateEngine(datac[:2], **component_engine_on).get()])\n",
    "print(a[:10])\n",
    "print(b[:10])\n",
    "print(a.__len__(), b.__len__())\n",
    "ut.TestCase().assertTrue(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit pd.DataFrame(data)[0].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit data_df[0].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e84ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VGrowthRateEngine(data).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VGrowthRateEngine(data1[:2], **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4735ae9",
   "metadata": {},
   "source": [
    "From the above observation, we can speculate that a pandas dataframe auto creates a stats table with pre shifted values. This is the only way pandas can be close to our vectorized performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f8ce8",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f90385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class VVarianceEngine(VectorizeEngine):\n",
    "    \"\"\"Rolling variance as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - use a rolling sum engine to perform a configurable computation\n",
    "                   - use this engine to perform a pure python vectorized rolling average\n",
    "               - Component Engine\n",
    "                   - the same as the final engine, except -\n",
    "                       - you must pass in pre prepared colum series for the rolling sum computation\n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__WINDOW = window\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        fS, fS2 = lambda *x: sum(*x), lambda x: sum([x**2 for x in x])\n",
    "        fV = lambda *x: (fS2(*x) - (fS(*x)**2 / self.__WINDOW)) / (self.__WINDOW - 1)\n",
    "        return [fV(x) for x in zip(*data)]\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            # raise RuntimeError('Variance component must be pure python.')\n",
    "            fS2 = lambda x: sum([x**2 for x in x])\n",
    "            ones = [1 for x in range(0, data[0].__len__())]\n",
    "            twos = [2 for x in range(0, data[0].__len__())]\n",
    "            ntotal = [self.__WINDOW for x in range(0, data[0].__len__())]\n",
    "            return self._divide([\n",
    "                self._subtract([\n",
    "                    [fS2(x) for x in zip(*data)],\n",
    "                    self._divide([\n",
    "                        self._power([\n",
    "                            self._add(data),\n",
    "                            twos\n",
    "                        ])\n",
    "                        , ntotal\n",
    "                    ])\n",
    "                ])\n",
    "                ,\n",
    "                self._subtract([ntotal, ones])\n",
    "            ])\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != self.__WINDOW or not self._is_matrices(data):\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly %s arrays' % self.__WINDOW\n",
    "                )\n",
    "            x = tuple(data.copy())\n",
    "        else:\n",
    "            if data.__len__() != 1 and not self._is_series(data):\n",
    "                raise ValueError(\n",
    "                    'data arary must be exactly 1 array as a python list ' +\n",
    "                    'type, and contain either a float or int'\n",
    "                )\n",
    "            x = self._vshift(data.copy(), 0, self.__WINDOW)\n",
    "        x = self._cropna(list(x))\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=list(x), row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f02540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var3(data, **kwargs):\n",
    "    return VVarianceEngine(data, 3, **kwargs).get()\n",
    "\n",
    "def var7(data, **kwargs):\n",
    "    return VVarianceEngine(data, 7, **kwargs).get()\n",
    "\n",
    "def var14(data, **kwargs):\n",
    "    return VVarianceEngine(data, 14, **kwargs).get()\n",
    "\n",
    "def var32(data, **kwargs):\n",
    "    return VVarianceEngine(data, 32, **kwargs).get()\n",
    "\n",
    "def var64(data, **kwargs):\n",
    "    return VVarianceEngine(data, 64, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tuple([round(x, 6) for x in VVarianceEngine(data, 7, dropna=True).get()])\n",
    "b = tuple([round(x, 6) for x in VVarianceEngine(datac, 7, **component_engine_on).get()])\n",
    "print(a[:10])\n",
    "print(b[:10])\n",
    "print(a.__len__(), b.__len__())\n",
    "ut.TestCase().assertTrue(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tuple([round(x, 4) for x in VVarianceEngine(datac, 7, **component_engine_on).get()])\n",
    "b = tuple([round(float(x), 4) for x in VVarianceEngine(datac, 7, use_pytorch_lib=pt, **component_engine_on).get()])\n",
    "c = tuple([round(float(x), 4) for x in VVarianceEngine(datac, 7, use_numpy_lib=np, **component_engine_on).get()])\n",
    "print(a[:10])\n",
    "print(b[:10])\n",
    "print(c[:10])\n",
    "print(a.__len__(), b.__len__(), c.__len__())\n",
    "ut.TestCase().assertTrue(a == b == c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877253b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VVarianceEngine(data, 7).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VVarianceEngine(datac, 7, use_pytorch_lib=pt, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VVarianceEngine(datac, 7, use_numpy_lib=np, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23502999",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit VVarianceEngine(datac, 7, **component_engine_on).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class VStandardDeviationEngine(VectorizeEngine):\n",
    "    \"\"\"Rolling standard deviation as a vectorized algorithm.\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   - use a rolling sum engine to perform a configurable computation\n",
    "                   - use this engine to perform a pure python vectorized rolling average\n",
    "               - Component Engine\n",
    "                   - the same as the final engine, except -\n",
    "                       - you must pass in pre prepared colum series for the rolling sum computation\n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array - single value array\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[array 1..n] - an array of arrays pre prepared\n",
    "\n",
    "       OUTPUT: array - a reduced vector result\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "            del kwargs['round_size']\n",
    "        kwargs.update({'round_size': None})\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__VRVAR = VVarianceEngine(data, window, **kwargs).get\n",
    "        self.__WINDOW = window\n",
    "\n",
    "    def _std(self, data):\n",
    "        V = data\n",
    "        return [math.sqrt(x) for x in V if x]\n",
    "\n",
    "    def pure_python_func(self, data):\n",
    "        return self._std(data)\n",
    "\n",
    "    def final_func(self, data):\n",
    "        if self._has_accel_lib():\n",
    "            raise RuntimeError('Average component must be pure python.')\n",
    "        else:\n",
    "            return self.pure_python_func(data)\n",
    "\n",
    "    def get(self):\n",
    "        data = [x for x in self.__VRVAR()]\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[], row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std3(data, **kwargs):\n",
    "    return VStandardDeviationEngine(data, 3, **kwargs).get()\n",
    "\n",
    "def std7(data, **kwargs):\n",
    "    return VStandardDeviationEngine(data, 7, **kwargs).get()\n",
    "\n",
    "def std14(data, **kwargs):\n",
    "    return VStandardDeviationEngine(data, 14, **kwargs).get()\n",
    "\n",
    "def std32(data, **kwargs):\n",
    "    return VStandardDeviationEngine(data, 32, **kwargs).get()\n",
    "\n",
    "def std64(data, **kwargs):\n",
    "    return VStandardDeviationEngine(data, 64, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = std3(data, round_size=7)\n",
    "b = tuple([round(x, 7) for x in list(data_df[0].rolling(3).std().values) if x > 0])\n",
    "print(a[-10:])\n",
    "print(b[-10:])\n",
    "print(a.__len__(), b.__len__())\n",
    "ut.TestCase().assertTrue(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f720db",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tuple([x for x in VStandardDeviationEngine(data, 7, round_size=None, dropna=True).get()])\n",
    "b = tuple([x for x in VStandardDeviationEngine(datac, 7, **component_engine_on).get()])\n",
    "print(a[:10])\n",
    "print(b[:10])\n",
    "print(a.__len__(), b.__len__())\n",
    "ut.TestCase().assertTrue(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8f9c1",
   "metadata": {},
   "source": [
    "# Inference Stats Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f134d",
   "metadata": {},
   "source": [
    "## Bayesian Inference Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ba704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from fractions import Fraction as F\n",
    "\n",
    "class OddsTools():\n",
    "    \"\"\" Methods to help with constructing odds related vectorizations.\n",
    "    \"\"\"\n",
    "    def __get_fall(self, x):\n",
    "        if x < 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def __get_rise(self, x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def get_growth_rate_as_tagged_tuple(self, series):\n",
    "        \"\"\" Use for vectorisation computations. A pre data format filter.\n",
    "\n",
    "            INPUT: array of growth rate values.\n",
    "            OUTPUT: a tag of growth rate values\n",
    "                    a tuple representing rise and fall for vectorisation computations\n",
    "                    (\n",
    "                        rise as value 1 other is 0,\n",
    "                        fall as value 1 other is 0\n",
    "                    )\n",
    "        \"\"\"\n",
    "        series_list = list(series)\n",
    "        series_size = series_list.__len__()\n",
    "        new = [(self.__get_rise(x), self.__get_fall(x)) for x in series_list if x]\n",
    "        dropped = series_size - new.__len__()\n",
    "        if dropped == 0:\n",
    "            return tuple(new)\n",
    "        return new\n",
    "\n",
    "\n",
    "tag = OddsTools().get_growth_rate_as_tagged_tuple\n",
    "\n",
    "\n",
    "class VDirectionalBayesianMatrixEngine(VectorizeEngine):\n",
    "    \"\"\"A 4 by 4 rolling vecotrized bayesian inference matrix ideal for time series inference.\n",
    "       Use it for deductive lock step, co-incidence, and seasonality.\n",
    "\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   -  \n",
    "               - Component Engine\n",
    "                   - the same as the final engine, except -\n",
    "                       - you must pass in pre prepared colum series for the rolling sum computation\n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array[[speculation grs], [benchmark grs]]\n",
    "                          - input series must be raw growth rate value +/- values\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[\n",
    "                                 [[speculation tagged tuples window],1...n],\n",
    "                                 [[benchmark tagged tuples window],1...n]\n",
    "                            ]\n",
    "                          - pre prepared\n",
    "                          - input series must be pre converted into a series of tuple tags\n",
    "\n",
    "              Generic Inference\n",
    "                    Evidence_tag - Evidence series in tagged form. E1 & E2 - Rise & Fall.\n",
    "                    Hypotheses_tag - Hypotheses series in tagged form. H1 & H2 - Rise & Fall.\n",
    "\n",
    "              Generic Growth Rate Inference\n",
    "                    SPtag - Speculation growth rate - wraps Evidence_tag\n",
    "                    BMtag - Benchmark growth rate - wraps Hypotheses_tag\n",
    "\n",
    "       OUTPUT: \n",
    "              Tuple of arrays containing the rolling posterior odds, given the sp and bm inputs.\n",
    "                  - (\n",
    "                      [P(H1|E1), P(H2|E1), P(H1|E2), P(H2|E2)],\n",
    "                      ...\n",
    "                    )\n",
    "\n",
    "              Generic Inference\n",
    "                    H1_E1 - P(H1|E1)\n",
    "                    H2_E2 - P(H2|E2)\n",
    "                    H2_E1 - P(H2|E1)\n",
    "                    H1_E2 - P(H1|E2)\n",
    "\n",
    "              Generic Growth Rate Inference\n",
    "                    BMrise_SPrise - wraps H1_E1\n",
    "                    BMfall_SPfall - wraps H2_E2\n",
    "                    BMfall_SPrise - wraps H2_E1\n",
    "                    BMrise_SPfall - wraps H1_E2\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._DATA = data\n",
    "        self._ROUND_SIZE = 6\n",
    "        if 'round_size' in kwargs.keys():\n",
    "            self._ROUND_SIZE = kwargs['round_size']\n",
    "        self._SKIP_POST_CLEANING = False\n",
    "        if 'skip_post_cleaning' in kwargs.keys() and kwargs['skip_post_cleaning']:\n",
    "            self._SKIP_POST_CLEANING = True\n",
    "            self._ROUND_SIZE = None\n",
    "        self._SKIP_PRE_PREPARATIONS = False\n",
    "        if 'skip_pre_preparations' in kwargs.keys() and kwargs['skip_pre_preparations']:\n",
    "            self._SKIP_PRE_PREPARATIONS = True\n",
    "        self.__WINDOW = window\n",
    "\n",
    "    def __min_zero(self, value):\n",
    "        if value == 0:\n",
    "            return 1\n",
    "        return value\n",
    "\n",
    "    def pure_python_func(self, SP_rolling, BM_rolling):\n",
    "        # Calculate Prior\n",
    "        #  - Create components\n",
    "        count_ones = lambda x: [i for i in x if i == 1].__len__()\n",
    "        rise = lambda x: [i[0] for i in x]\n",
    "        fall = lambda x: [i[1] for i in x]\n",
    "        sp_r = [rise(x) for x in zip(*SP_rolling)]\n",
    "        sp_f = [fall(x) for x in zip(*SP_rolling)]\n",
    "        bm_r = [rise(x) for x in zip(*BM_rolling)]\n",
    "        bm_f = [fall(x) for x in zip(*BM_rolling)]\n",
    "        intersect = lambda s, b: [x * y for x, y in zip(s, b)]\n",
    "        #  - Reduce into BM quadrant components\n",
    "        #  - multiply the rolling rises and falls\n",
    "        p_h1_e1 = [intersect(s, b) for s, b in zip(sp_r, bm_r)]\n",
    "        p_h2_e2 = [intersect(s, b) for s, b in zip(sp_f, bm_f)]\n",
    "        # - count the rolling ones\n",
    "        bm_p_h1_e1 = [count_ones(x) for x in p_h1_e1]\n",
    "        bm_p_h2_e2 = [count_ones(x) for x in p_h2_e2]\n",
    "        #  - multiply the rolling rises and falls\n",
    "        p_h1_e2 = [intersect(s, b) for s, b in zip(sp_f, bm_r)]\n",
    "        p_h2_e1 = [intersect(s, b) for s, b in zip(sp_r, bm_f)]\n",
    "        # - count the rolling ones\n",
    "        bm_p_h1_e2 = [count_ones(x) for x in p_h1_e2]\n",
    "        bm_p_h2_e1 = [count_ones(x) for x in p_h2_e1]\n",
    "\n",
    "        # Calculate Posterior\n",
    "        f = lambda x: (\n",
    "            F(self.__min_zero(x[0]) / self.__min_zero(x[1])).limit_denominator(100),  # P(H1|E1) / P(H2|E1)\n",
    "            F(self.__min_zero(x[1]) / self.__min_zero(x[0])).limit_denominator(100),  # P(H2|E1) / P(H1|E1)\n",
    "            F(self.__min_zero(x[2]) / self.__min_zero(x[3])).limit_denominator(100),  # P(H1|E2) / P(H2|E2)\n",
    "            F(self.__min_zero(x[3]) / self.__min_zero(x[2])).limit_denominator(100)   # P(H2|E2) / P(H1|E2)\n",
    "        )\n",
    "        # - left to right - top to bottom - matrix\n",
    "        bm = [\n",
    "            f(x) for x in zip(\n",
    "                bm_p_h1_e1, bm_p_h2_e1,\n",
    "                bm_p_h1_e2, bm_p_h2_e2\n",
    "            )\n",
    "        ]\n",
    "        return bm\n",
    "\n",
    "    def final_func(self, data):\n",
    "        SP = data[0]\n",
    "        BM = data[1]\n",
    "        if self._has_accel_lib():\n",
    "            raise RuntimeError('BayesianMatrix component must be pure python.')\n",
    "        else:\n",
    "            return self.pure_python_func(SP, BM)\n",
    "\n",
    "    def get_rolling_data(self):\n",
    "        data = self._DATA\n",
    "        if data.__len__() != 2 or not self._is_series(data[0]):\n",
    "            raise ValueError(\n",
    "                'data array must contain exactly 2 arrays containing series ' +\n",
    "                'growth rate values +/-'\n",
    "            )\n",
    "        sp = self._vshift(tag(gr(list(data[0]).copy())), 0, self.__WINDOW)\n",
    "        bm = self._vshift(tag(gr(list(data[1]).copy())), 0, self.__WINDOW)\n",
    "        return sp, bm\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != 2 or data[0][0][0].__len__() != self.__WINDOW:\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly 2 arrays containing rolling ' +\n",
    "                    'window arrays of %s values' % self.__WINDOW\n",
    "                )\n",
    "            sp = tuple(data[0].copy())\n",
    "            bm = tuple(data[1].copy())\n",
    "        else:\n",
    "            sp, bm = self.get_rolling_data()\n",
    "        sp, bm = self._cropna(sp), self._cropna(bm)\n",
    "        if not self._SKIP_POST_CLEANING:\n",
    "            return self._square_right_by_ref(\n",
    "                self._DATA[1],\n",
    "                self.ignite(\n",
    "                    data, self.final_func, columns=[sp, bm], row_funcs=[],\n",
    "                    round_size=self._ROUND_SIZE\n",
    "                )\n",
    "            )\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[sp, bm], row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bme(data, window, **kwargs):\n",
    "    return VDirectionalBayesianMatrixEngine(data, window, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case from Speculative Statistics III CH4.1\n",
    "spx = [1,2,3,2,3,2,1]\n",
    "bmx = [1,2,3,4,3,4,5]\n",
    "sp = data.copy()\n",
    "sp.extend(spx)\n",
    "bm = datax.copy()\n",
    "bm.extend(bmx)\n",
    "#print(tag(gr(sp)))\n",
    "rtn = bme((sp, bm), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23567320",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_matrix = rtn[-1:][0]\n",
    "print(last_matrix)\n",
    "print(F(last_matrix[0]).limit_denominator(100))\n",
    "print(F(last_matrix[1]).limit_denominator(100))\n",
    "print(F(last_matrix[2]).limit_denominator(100))\n",
    "print(F(last_matrix[3]).limit_denominator(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bm.__len__())\n",
    "print(rtn.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80603325",
   "metadata": {},
   "source": [
    "## Lock Step Bayesian Inference Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class VLockStepBayesianMatrixEngine(VDirectionalBayesianMatrixEngine):\n",
    "    \"\"\"Rolling lock step bayesian matrix as a vectorized algorithm.\n",
    "\n",
    "       DESIGN: - Final Engine - Default Configured\n",
    "                   -  \n",
    "               - Component Engine\n",
    "                   - the same as the final engine, except -\n",
    "                       - you must pass in pre prepared colum series for the rolling sum computation\n",
    "                \n",
    "               - \n",
    "\n",
    "       INPUT: FINAL ENGINE - DEFAULT\n",
    "              kwargs - skip_pre_preparations=False, skip_post_cleaning=False\n",
    "                     data - array[[speculation grs], [benchmark grs]]\n",
    "                          - input series must be raw growth rate value +/- values\n",
    "\n",
    "              COMPONENT ENGINE\n",
    "              kwargs - skip_pre_preparations=True, skip_post_cleaning=True\n",
    "                     data - array[\n",
    "                                 [[speculation tagged tuples window],1...n],\n",
    "                                 [[benchmark tagged tuples window],1...n]\n",
    "                            ]\n",
    "                          - pre prepared\n",
    "                          - input series must be pre converted into a series of tuple tags\n",
    "\n",
    "              Generic Inference\n",
    "                    Evidence_tag - Evidence series in tagged form. E1 & E2 - Rise & Fall.\n",
    "                    Hypotheses_tag - Hypotheses series in tagged form. H1 & H2 - Rise & Fall.\n",
    "\n",
    "              Generic Growth Rate Inference\n",
    "                    SPtag - Speculation growth rate - wraps Evidence_tag\n",
    "                    BMtag - Benchmark growth rate - wraps Hypotheses_tag\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window, lag, **kwargs):\n",
    "        super().__init__(data, window, **kwargs)\n",
    "        self.__WINDOW = window\n",
    "        if lag <= 0:\n",
    "            raise ValueError('Lag must be 1 or more for lock step calculations.')\n",
    "        self.__LAG = lag\n",
    "\n",
    "    def get_rolling_data(self):\n",
    "        data = self._DATA\n",
    "        if data.__len__() != 2 or not self._is_series(data[0]):\n",
    "            raise ValueError(\n",
    "                'data array must contain exactly 2 arrays containing series ' +\n",
    "                'growth rate values +/-'\n",
    "            )\n",
    "        # Co-incide the previous value for bayesian matrix intersection calculations, ie. lock step\n",
    "        dropna = self._dropna\n",
    "        if self.__LAG == 1:\n",
    "            sp = dropna(data[0])\n",
    "            sp = self._vshift(tag(gr(list(sp).copy())), 0, self.__WINDOW)\n",
    "\n",
    "        elif self.__LAG > 1:\n",
    "            sp = self._dropna(data[0])\n",
    "            sp = self._vshift(\n",
    "                tag(\n",
    "                    dropna(\n",
    "                        cumsum(\n",
    "                            dropna(\n",
    "                                gr(sp)\n",
    "                            ),\n",
    "                            self.__LAG\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                0,\n",
    "                self.__WINDOW\n",
    "            )\n",
    "\n",
    "        bm = self._vshift(tag(gr(list(data[1]).copy())), 0, self.__WINDOW)\n",
    "        rtn = self._square_right([sp, bm])\n",
    "        return rtn[0], rtn[1]\n",
    "\n",
    "    def get(self):\n",
    "        data = self._DATA\n",
    "        if self._SKIP_PRE_PREPARATIONS:\n",
    "            if data.__len__() != 2 or data[0][0][0].__len__() != self.__WINDOW:\n",
    "                raise ValueError(\n",
    "                    'data array must contain exactly 2 arrays containing rolling ' +\n",
    "                    'window arrays of %s values' % self.__WINDOW\n",
    "                )\n",
    "            sp = tuple(data[0].copy())\n",
    "            bm = tuple(data[1].copy())\n",
    "        else:\n",
    "            sp, bm = self.get_rolling_data()\n",
    "        sp, bm = self._cropna(sp), self._cropna(bm)\n",
    "        if not self._SKIP_POST_CLEANING:\n",
    "            return self._square_right_by_ref(\n",
    "                self._DATA[1],\n",
    "                self.ignite(\n",
    "                    data, self.final_func, columns=[sp, bm], row_funcs=[],\n",
    "                    round_size=self._ROUND_SIZE\n",
    "                )\n",
    "            )\n",
    "        return self.ignite(\n",
    "            data, self.final_func, columns=[sp, bm], row_funcs=[],\n",
    "            round_size=self._ROUND_SIZE\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ec0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bme_lockstep(data, window, lag, **kwargs):\n",
    "    return VLockStepBayesianMatrixEngine(data, window, lag, **kwargs).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbe387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test case from Speculative Statistics III CH4.1 but results are for lock step\n",
    "spx = [1,2,3,2,3,2,1]\n",
    "bmx = [1,2,3,4,3,4,5]\n",
    "sp = data.copy()\n",
    "sp.extend(spx)\n",
    "bm = datax.copy()\n",
    "bm.extend(bmx)\n",
    "#print(tag(gr(sp)))\n",
    "rtn = bme_lockstep((sp, bm), 64, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_matrix = rtn[-1:][0]\n",
    "print(last_matrix)\n",
    "print(F(last_matrix[0]).limit_denominator(100))\n",
    "print(F(last_matrix[1]).limit_denominator(100))\n",
    "print(F(last_matrix[2]).limit_denominator(100))\n",
    "print(F(last_matrix[3]).limit_denominator(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f851d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sp.__len__())\n",
    "print(rtn.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac2075",
   "metadata": {},
   "source": [
    "## Seasonal Bayesian Inference Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use bme or bme_lockstep but with pre created seasonal tags via rlnp\n",
    "import math\n",
    "import rlab_common_numpy_pandas as rlnp\n",
    "\n",
    "# Validate series as time series\n",
    "\n",
    "\n",
    "\n",
    "# Get seasonal bme and bme_lockstep\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch as pt\n",
    "tf.__name__\n",
    "pt.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102996b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0815180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7c397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa7f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f53d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe60f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99385ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447feb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad46926",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545b317",
   "metadata": {},
   "source": [
    "Vectorization is fast.<br/>\n",
    "\n",
    "A slow vectorization is usually caused by unecessary data preparation and cleaning, pre and post computation.<br/>\n",
    "\n",
    "The ideal solution is to separate a mass computional orchestration into stages so that expensive coding operations are done in bulk and not on every vectorization call. <br/>\n",
    "\n",
    "**Organisational Orchestration Stages -**\n",
    "* **Clean** - type conversions\n",
    "* **Prepare** - series duplication and value shifting\n",
    "* **Compute** - Component Engines - Fine Grained\n",
    " * Multiple vectorization engines reducing and enriching result\n",
    " * Must have data preparation and cleaning turned off\n",
    "* **Compute** - Final Engines - Course Grained\n",
    " * Takes all the results from the mass component computations and reduces it to a final result\n",
    " * **Clean** - round the reduced result\n",
    "* **Store**\n",
    " * Store output of engine components to flat file\n",
    " * Store output of final engine compute to flat file\n",
    "<br/>\n",
    "\n",
    "With the above mass orchestration, **we do not duplicate** expensive data cleaning and preparations within each vectorization engine. <br/>\n",
    "\n",
    "We also are enabled to test cycle the final engine's algorithm by storing the component engine's result. <br/>This enables fast failure cycles for the final compute R&D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
